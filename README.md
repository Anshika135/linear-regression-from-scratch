# ğŸš€ Linear Regression From Scratch (NumPy Only)

A complete implementation of **Linear Regression built from scratch using only NumPy**, without using scikit-learn or any ML libraries.

This project focuses on understanding the **mathematics and optimization behind machine learning algorithms**, including both **Gradient Descent** and **Ordinary Least Squares (Normal Equation)**.

---

## âœ¨ Features

- âœ… Linear Regression using Gradient Descent
- âœ… Linear Regression using OLS (Normal Equation)
- âœ… Fully vectorized NumPy implementation
- âœ… No sklearn or external ML libraries
- âœ… Clean and modular code
- âœ… sklearn-like API (`fit()`, `predict()`)
- âœ… Beginner-friendly and interview-ready

---

## ğŸ“š Algorithms Implemented

### ğŸ”¹ Gradient Descent
An iterative optimization algorithm that updates weights step-by-step to minimize Mean Squared Error.

**Best for:**
- Large datasets
- Many features
- When matrix inversion is expensive

**Update rule:**
w = w - Î± * dJ/dw


---

### ğŸ”¹ Ordinary Least Squares (OLS) â€“ Normal Equation
A closed-form mathematical solution using linear algebra.

**Formula:**
Î¸ = (Xáµ€X)â»Â¹Xáµ€y
